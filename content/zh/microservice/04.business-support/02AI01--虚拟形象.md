---
title: "02 AI 01 数字虚拟形象"
description: "业务支持"
date: 2023-04-22T12:52:56+08:00
lastmod: 2023-04-22T12:52:56+08:00
draft: false
images: []
menu:
  microservice:
    parent: "04.business-support"
weight: 20402
toc: true
---

## 0. 概述

伴随着人工智能技术的不断发展，语音、手势、触控、人脸等，多模态交互也全面开启智能时代。以虚拟形象为主的交互方式已经成为了交互的前沿，综合了智能交互的各种方式。虚拟形象可以应用在各种场景下，例如：虚拟课堂，虚拟会议，虚拟主播，虚拟客服等等场景。

虚拟形象是对各种人工智能的综合使用场景，现阶段云厂商基本都提供了虚拟形象的服务。但现阶段云厂商提供的虚拟形象服务都以黑盒的方式提供，这样使使用方无法评估云厂商提供的虚拟形象的可用性，适配性，健壮性，分发能力等等。之所以云厂商不将底层技术实现描述清晰，也是因为虚拟形象技术发展还处于初期阶段。技术初期阶段到技术稳定器肯定会伴随着技术的重大变更。

在虚拟形象发展初期云厂商提供的虚拟形象服务，对于技术的抽象以及提供的开放接口形态都不甚完善。也造成在技术逐渐成熟过程中会有接口的重大变更。

本文为了解决技术黑盒与接口抽象问题，从虚拟形象技术解决方案中可能涉及到的人工智能、技术、逻辑来说明虚拟形象的分类以及技术特点。从而可以更有效的评估虚拟形象服务是否适合。

## 1. 虚机形象分类

### 0. 描述

虚拟形象即在UI上展示出数字形象进行表演、沟通等行为。所以在这里要求分几个层次:

1. 可以进行多轮对话(非重点)
2. 可以展示出数字形象
3. 数字形象可以根据要求进行相应的动作
4. 可以根据说话内容对上唇形
5. 可以根据情绪，展示出不同的表情

虚拟人概念比较广泛，在业务与技术调研过程中会发现很多地方都讲多轮对话作为虚拟人来称呼。其实用于多轮对话的模型是虚拟形象领域中的交互核心，而不是全部。

虚拟形象包含多轮对话、数字形象、情感迁移三项，以及在组成数字形象解决方案时用于链接三项的能力。完善的虚拟形象解决方案包括很多内容，但并不是所有的业务场景都需要完整的虚拟形象解决方案。例如：在虚拟课程业务场景下通过演示PPT和语音之间同步是最重要的，而数字形象的存在并不是必须的。在虚拟课程场景下数字形象、数字形象的语音唇形的优先级就低于声音与演示PPT同步功能。

不同的应用场景，需要的功能也是不一样的。对于功能的业务优先级也不一样。所以在各种数字形象的解决方案中并不一定包含所有的功能。

### 1. 虚拟形象分类

在进行虚拟形象评估之前需要明确很多业务问题，最主要的是确认数字形象到底要提供哪些服务？如果不是现在理不清要要在那个场景下使用虚拟形象，可以回答以下几个问题：
1. 必须以数字形象的方式进行交互？还是语音交互就可以？甚至只用文字交互就可以？
2. 对所有虚拟形象的观看者，还是对单个观看者的虚拟形象。
3. 虚拟形象是用来生成重复播放的视频？还是根据观看者的特点而独立生成的？
4. 需要特定的领域中的问答集？还是通用的聊天场景？

回答完以上的这些问题，就可以从下面分类中选择不同的类型。

#### 用途分类
1. **形象跟踪**

    主要跟踪面部动作、肢体动作并在数字形象中实时展示出的场景。这种技术应用比较多的是影视视频制作，很多电影中都用到了该技术。
    但，这个类型的虚拟形象不会与其他的AI技术对接。基本上就是生成一段视频，并发布即可。
    - 分类
        - 表情驱动
        - 肢体驱动

2. **合成视频类**

    通过一段输入，生成合适的视频。并发布。
    对比“形象跟踪”这里通过人工智能生成数字形象的动作、表情、视频场景等内容。
    - 分类
        - 文本驱动
        - 声音驱动

3. **模型控制类**

    3D/2D模型直接在UI上展示，并根据输入控制UI上的形象做相应的动作、表情的过程。
    - 分类
        - 浏览器模型驱动
        - 游戏模型驱动
        - 移动端模型驱动

#### 技术分类
1. **视频类**

    视频类即虚拟形象以视频的方式呈现。
    - 视频文件

        最终生成视频文件，可以多次播放视频。
        例如：视频主播上传到视频网站中的视频。这种视频可以随时从头播放到尾。

    - 实时视频流

        对外部输入形成实时的反馈的视频。
        例如：视频主播的在线直播，每个观看者进入主播房间后都看到的是主播当前表演的内容。

2. **模型驱动类**

    模型驱动类即以3D/2D模型方式呈现，并可以通过控制模型动作、表情、场景来展示动画。**特点：** 可以详细的控制每个客户看到的内容都是不一样的。因为与虚拟形象交互的每个客户都可以对虚拟形象提特有的问题，而虚拟形象可以根据不同客户的问题形成不同的答案而展示出不同的动作、表情。

## 2. 制作过程

### 0. 描述
前面讨论了虚拟形象适用于哪些业务场景，也明确了场景下可以使用哪些类型的虚拟形象。有了业务场景之后，就需要具体实施虚拟形象。本节讨论实施阶段要完成的工作。用来指导实际虚拟形象研发过程。

从研发过程中可以了解到虚拟形象为了满足业务场景中的需求而在背后做的工作，以深入了解虚拟形象解决方案以及调整方式。更好的为虚拟形象运营提供指导。

### 1. 过程
1. 确定具体的业务场景

    回答完上节中关于业务场景的问题，并选择了使用哪一类的虚拟形象。就可以决定后面的几个步骤具体应该怎样去做。

2. 制定话术或者问答集
    - **目标**：
        满足虚拟形象与使用者之间沟通过程中的可沟通范围，以及虚拟形象可以回答的标准话术。
        包括话术的情感偏向都可以在这里进行制定。
    - **内容**：
        针对场景的话术或者问答集合。
        制定寒暄语，以及内容。

3. 制作虚拟形象
    - **目标**：
        构建虚拟形象，以及虚拟形象所处的场景。以虚拟形象和场景的方式为业务场景提供支撑。
    - **内容**：
        - 人种、语种

            人种与语种有一些对应关系，在特定对应关系下会有更自然、流畅的沟通过程。

        - 背景、动作

            根据场景，在虚拟形象的背景以及虚拟形象的动作都会有不同的选择。例如：智能客服会选择纯色或者营业柜台的方式，让用户带入具体的场景。

        - 使用的3D模型、2D模型，进行模型的构建

            （非必须）捕捉面部动作，捕捉身体动作，以训练人工智能

4. 驱动虚拟形象动作与语音
    - **目标**：以合适的方式来驱动虚拟形象
    - **内容**：
        - 驱动方式的选择
            视频
            模型控制
        - 语音选择
            男女声
            背景音
            声音的年龄
        - 情感迁移
            声音的感情
            面部表情的感情
            肢体动作的感情

## 3. 实现技术

### 1. 技术过程

在技术实现的过程中也包括需要前期构建好的工作，以及运行期的流程。

1. **建模过程**

    - 形象建模：**虚拟形象3D/2D模型**、**背景选择**。
    - 声音建模、对话建模：**预训练模型的微调**、**语言分类模型的选择**

2. **运行过程**

    在虚拟形象实际运行过程中，每个与虚拟形象的交互都是按照下面的流程一步一步处理的。在每一个步骤中完成一项或多项工作，多项工作时需要并行处理以提高性能。

    - Step 0. 
        - 目标：虚拟形象与用户之间的交互
        - 方式：收音、拍摄、文字
    - Step 1.
        - 目标：获取输入内容中的各种信息
        - 工作：语种识别、语音识别、情绪分析
    - Step 2.
        - 目标：生成交互结果
        - 工作：自然语言处理、情感迁移
    - Step 3.
        - 目标：生成语音结果
        - 工作：语音合成、情感迁移
    - Step 4.
        - 目标：让虚拟形象以更自然的方式进行交互
        - 工作：声音驱动

### 2. 技术选项
#### 展示模型

展示模型就是为了在界面上展示虚拟形象的模型，并不是人工智能模型。展示模型要可以展示与动作。展示模型的构建使用3D/2D建模软件进行。在建模过程中需要有以下内容，方便在运营过程中控制虚拟形象动作。

- Blendshapes：Blendshapes泛指3D定点动画的制作方式 (Maya里面称之为 blend shapes ，而3DS Max里称之为morph targets) ，在3D动画中用的比较多，尤其是人脸动画的制作，通过blendshape来驱动角色的面部表情。
    - 定位符：面部节点，骨骼节点
    - 基于Blendshapes的技术更容易在客户端上进行模型动作的操控。
- Talking Head/Digital Hunman：说话头像和数字人
- VRM：VRM 是 3D 虚拟化身数据的标准档案格式。VRM 虚拟化身具备以下优势：跨平台支援各式各样的应用程式，为一种整合性资料格式，内含所需的 3D 网格、纹理、骨架和元数据等数据，精确的脸部动作数据，可做出多种脸部表情，自然流畅的人物动画应用

技术：
- **构建**

    都是使用3D建模工具进行建模动作，这方面最专业的还是影视制作、游戏制作公司比较专长。可以引入3D建模的领域专家进行建模过程。

- **运行**
    - **视频合成**
        - Live2D
            - **特点**：Live2D是一种能做出2D电脑图形的动画软件，生成的人物通常是日系动画造型。Live2D使得角色能够使用2.5D的方式来移动，同时以低成本保持原画的外观和画风，无需逐帧制作动画或3D模型。它可以被认为是动画的成本和效果的平衡。Live2D由日本程序员中城哲也发明。 Live2D角色由分成不同的图层所组成。
            - **地址**：[Live2D](https://www.live2d.com/)
        - SadTalker
            - **特点**：SadTalker就是根据一张图片、一段音频，合成面部说这段语音的视频。可以直接在 Stable Diffusion 上使用。
                CVPR 2023论文对应的开源项目，是由西安交通大学、腾讯AI Lab和蚂蚁集团共同发布的。
            - **地址**：[SadTalker](https://github.com/OpenTalker/SadTalker)
        - ER-NeRF
            - **特点**：ER-NeRF，是一种新的基于条件神经辐射场（NeRF）的用于说话人像合成的架构，可以在小模型尺寸的情况下同时实现快速收敛、实时渲染和最先进的性能。
                ICCV 2023论文由北京航空航天大学、格里菲斯大学、RIKEN AIP、东京大学共同发布。
            - **地址**：[ER-NeRF](https://fictionarry.github.io/ER-NeRF/)
    - **视频流合成**

        在ER-NeRF中可以使用逐帧渲染功能，以每帧图片做为视频流中的一帧。

    - **模型控制**
        - Godot Engine
            - **特点**：
                一款开源的游戏引擎，适用于2D和3D游戏开发，可以用于创建数字虚拟形象。
                可以使用动画模式控制节点
            - **地址**：[Godot Engine](https://godotengine.org/)
        - Three.js
            - **特点**：
                使用GLTF方式进行模型的加载与渲染，Animation使用Blendshapes方式进行模型动作的驱动。
            - **地址**：[Three.js](https://threejs.org/)
        - Blender
            - **特点**：是一款功能强大的开源3D计算机图形软件，可用于建模、动画制作和渲染。
                可以使用动画模式控制节点
            - **地址**：[Blender](https://www.blender.org/)

#### 声音模型

声音模型需要解决两个问题：声音转文字，文字转声音。

- **ASR（语音识别）**
    - **自然语言处理输入参数**

        在虚拟形象与使用方进行交互过程中，需要对使用方的交互内容进行分析和答案生成过程。而这个分析与生成过程都是使用文本的方式进行的。所以需要将声音文件转化为文字。

    - **监控与优化**

        而为了监控与改进交互过程，需要将交互的过程进行记录。记录使用音频文件时会占用过多的存储空间，所以最好使用文本方式进行记录

    基于以上两个原因，需要进行语音识别过程。
- **TTS（合成语音）**
    - **音色选择**

        像个人形象一样，个人的声音也受到法律保护。所以在使用声音合成过程中，也需要付声音使用费。而在这个过程中为了避免侵权，最好是根据场景与特点自己训练声音合成模型。
    - **参数**：情感、音量、语速、音高

技术：
- **构建**
    - **生成模型训练**
        - Generative Voice AI
            - **地址**：[elevenlabs](https://elevenlabs.io/)、[Revocalize](https://Revocalize.ai)
        - Hugging Face
            - **特点**：在huggingface上的绝大部分TTS模型都可以进行声音克隆
    - **音色转换STSC**

        音色转换使用深度学习算法、声音特征提取、情感节奏模拟等多项人工智能技术，在保留原始音频的节奏、情感、语气等特征的前提下，实现音色的精准迁移。

- **运行**
    - **语音识别**
        - openai/whisper-large-v3 
            - **特点**：Whisper 是用于自动语音识别 (ASR) 和语音翻译的预训练模型。Whisper 模型经过 68 万小时的标记数据训练，表现出强大的泛化能力，无需进行微调即可推广到许多数据集和领域。
            - **地址**：[openai/whisper-large-v3](https://huggingface.co/openai/whisper-large-v3)
    - **合成语音**
        - XTTS-v2模型
            - **特点**：TTS 是一种语音生成模型，让您只需使用 6 秒的快速音频剪辑即可将语音克隆为不同的语言。不需要跨越无数小时的大量训练数据。
            - **地址**：[coqui/XTTS-v2](https://huggingface.co/coqui/XTTS-v2)
        - vits模型
            - **特点**：VITS（Variational Inference with adversarial learning for end-to-end Text-to-Speech）是一种结合变分推理（variational inference）、标准化流（normalizing flows）和对抗训练的高表现力语音合成模型。VITS通过隐变量而非频谱串联起来语音合成中的声学模型和声码器，在隐变量上进行随机建模并利用随机时长预测器，提高了合成语音的多样性，输入同样的文本，能够合成不同声调和韵律的语音。

#### 语音音素（phoneme）映射到面部表情
调研过程中会发现音素到视素之间的映射关系这部分仍然处于科研前沿。并未形成完善的可商业化使用的技术，故在这里可以看到很多都是以论文的形式存在的。

- 唇形生成
    - 合成视频
        - Audio2Face
            - **特点**：Audio2Face是一个开源的语音驱动表情技术项目，通过语音信号分析和机器学习技术，实现数字人的语音交互功能。
            - **地址**：[Omniverse Audio2Face 结合Metahuman的使用](https://zhuanlan.zhihu.com/p/463827738)
        - Video-Tetalking
            - **特点**：Video-Tetalking是一个开源的音视频同步项目，可用于实现数字人的口型与语音的同步，情绪表达能力。西安电子科技大学，腾讯人工智能实验室，清华大学的研究成果。
            - **地址**：[Video-Tetalking](https://github.com/vinthony/video-retalking.git)
        - Wav2lip
            - **特点**：
                Wav2Lip实现的是视频人物根据输入音频生成与语音同步的人物唇形，使得生成的视频人物口型与输入语音同步。
                Wav2Lip不仅可以基于静态图像来输出与目标语音匹配的唇形同步视频，还可以直接将动态的视频进行唇形转换，输出与目标语音匹配的视频。
                Wav2Lip实现唇形与语音精准同步突破的关键在于，它采用了唇形同步判别器，以强制生成器持续产生准确而逼真的唇部运动。此外，它通过在鉴别器中使用多个连续帧而不是单个帧，并使用视觉质量损失（而不仅仅是对比损失）来考虑时间相关性，从而改善了视觉质量。Wav2Lip适用于任何人脸、任何语言，对任意视频都能达到很高都准确率，可以无缝地与原始视频融合，还可以用于转换动画人脸。
                wav2lip模型的训练分为两个阶段，第一阶段是专家音频和口型同步判别器预训练；第二阶段是GAN网络训练。
            - **地址**：[Wav2lip](https://github.com/Rudrabha/Wav2Lip)
    - 音素2视素
        - Viseme
            - **特点**：微软提供的服务，与TTS集成的
            - **地址**：[Viseme](https://learn.microsoft.com/zh-tw/azure/ai-services/speech-service/how-to-speech-synthesis-viseme?tabs=visemeid&pivots=programming-language-javascript)
        - Amazon Polly
            - **特点**：Amazon提供的服务，与TTS集成的
            - **地址**：[Amazon Polly](https://aws.amazon.com/cn/polly/)
        - FaceWare Live SDK
            - **特点**：FaceWare Technologies 提供的 FaceWare Live SDK 允许开发人员将面部动画与语音同步，实现面部表情的动态生成。
        - Apple Animoji/Memoji
            - **特点**：在苹果设备上，Animoji 和 Memoji 是一种使用 TrueDepth 摄像头捕捉用户面部表情并将其应用于3D表情符号或用户创建的虚拟角色的技术。
        - EmotionML
            - **特点**：Emotion Markup Language（EmotionML）是一个由万维网联盟（W3C）定义的标准，旨在描述情感和表情的信息。它可以与其他标准结合使用，以支持更广泛的面部表情和情感表示。
- 身体形态展示
    - Speech Gestures（语音手势）
        - **特点**：一些研究团队致力于将语音与手势和面部表情相结合，以创建更加综合的表达方式。这包括使用深度学习和计算机视觉技术。
    - 其他
        - FreeMo：一个能自动生成上半身手势的模型，以响应语音。提出了一种基于姿势模式分支和节奏运动分支的生成方法，与以往的手势生成模型不同。
        - 音频到身体动力学（Audio2Body）：采用RNN网络进行语音到手势的转换。
        - Speech2Gesture（S2G）：采用CNN网络从语音中生成手势。
        - 语音驱动模板（Tmpt）：学习手势模板，以解决从语音到身体动作的映射中的模糊问题
        - 三元语境（TriCon）：采用RNN网络，从三个输入中学习：语音、文本和SpeakerID。
        - Mix-StAGE：一个生成模型，为每个说话人学习独特的风格嵌入。
#### 多轮对话

虚拟形象沟通核心逻辑是自然语言处理，而产品化的虚拟形象需要支持多种语言，多种场景。基于产品化需求多轮对话需要有语种识别能力、需要可以进行语种之间的翻译、需要支持支持多轮对话的自然语言处理。

- 语种识别
    - 文本语种识别
        - fasttext
            - **特点**：简单、快捷的语种识别工具，多语种识别
            - **地址**：[fasttext](https://fasttext.cc/)
    - 语音语种识别
        - speechbrain/lang-id-voxlingua107-ecapa 
            - **特点**：简单、快捷的语种识别工具，多语种识别
            - **地址**：[speechbrain/lang-id-voxlingua107-ecapa](https://huggingface.co/speechbrain/lang-id-voxlingua107-ecapa)
- 翻译

    一般选择云翻译服务进行翻译，这样可以达到商用级的翻译能力。并且费用也不会很高。

- 自然语言处理

    可以使用一套模型完成自然语言处理，也可以将自然语言分离使用。这个根据不同的需求进行，智能客服类可以分开，而问答类可以使用一个模型完成这两件事情。

    - 自然语言识别
        - facebook/bart-large-mnli
            - **特点**：使用BART模型进行自然语言识别，并且可以进行文本分类。
            - **地址**：[facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli)
    - 自然语言生成
        - BERT（Bidirectional Encoder Representations from Transformers）
            - **特点**：BERT 是一种预训练语言表示的方法，这意味着我们在大型文本语料库（如维基百科）上训练通用的“语言理解”模型，然后将该模型用于我们关心的下游 NLP 任务（如问题）回答）。
            - **地址**：[BERT](https://huggingface.co/bert-base-multilingual-cased)

#### 情绪识别

在Hugging Face上有很多开源的情绪分析模型，有分析不同语种、不同情绪分类、基于文字、基于语音、基于图像的情绪分析模型。也有不同正确率的情绪分析模型。可以在Hugging Face模型的文本分类、语音分类中找到这种模型。

情感迁移需要配合自然语言生成、TTS模型来进行，例如TTS中的coqui/XTTS-v2可以根据输入的微调音频进行情感迁移。

#### 动作捕捉（MoCap）
对于想虚拟UP这样的需求来说，以数字形象替代个人形象并根据自己的实时动作进行展示是一种虚拟形象的需求。可以通过行为跟踪人工智能来实现。在CG影视后期的动作捕捉后需要渲染合成之后形成视频，而人工智能以Blendshapes的识别以表级作为基础，作为驱动模型的方式。动作捕捉可以分为：面部跟踪、身体形态跟踪、手势跟踪。

从人工智能的角度来描述这部分工作应该是人脸数据的特征工程。

人脸识别算法主要包含三个模块：
- **人脸检测（Face Detection）**：
    人脸检测用于确定人脸在图像中的大小和位置，即解决“人脸在哪里”的问题，把真正的人脸区域从图像中裁剪出来，便于后续的人脸特征分析和识别。
- **人脸特征表征（Feature Representation）**：
    人脸特征提取，也叫人脸表征，是对人脸进行特征建模的过程。 人脸特征提取的方法主要分为两大类：一类是基于知识的表征方法；另一类是基于代数特征或统计学习的表征方法。 以知识为基础的表征方法主要是根据人脸器官的形状描述及其相互间的距离特征，获取特征点的欧氏距离、曲率、角度等特征进行分类。
- **人脸对齐（Face Alignment）**：
    同一个人在不同的图像序列中可能呈现出不同的姿态和表情，这种情况是不利于人脸识别的。所以有必要将人脸图像都变换到一个统一的角度和姿态，这就是人脸对齐。
    它的原理是找到人脸的若干个关键点（基准点，如眼角，鼻尖，嘴角等），然后利用这些对应的关键点通过相似变换（Similarity Transform，旋转、缩放和平移）将人脸尽可能变换到标准人脸。

具体技术（以可以进行的跟踪类型分类）：
- 面部跟踪
    - face-alignment
        - **特点**：
            使用世界上最准确的面部对齐网络从 Python 检测面部标志，能够检测 2D 和 3D 坐标中的点。
            主要做人脸对齐使用。但可以进行人脸定位符的检测。
            可以根据人脸定位符，在训练表情识别过程。所以，可以支持情感分析过程。
        - **地址**：[face-alignment](https://github.com/1adrianb/face-alignment)
    - facexlib
        - **特点**：facexlib是一个基于pytorch的库，用于人脸相关功能，例如检测、对齐、识别、跟踪、人脸修复的utils等。
        - **地址**：[facexlib](https://github.com/xinntao/facexlib)
- 多跟踪
    - MediaPipe
        - **特点**：
            MediaPipe 包含您轻松自定义和部署到移动设备（Android、iOS）、Web、桌面、边缘设备和物联网所需的一切。
            具有简单易用抽象的自助式机器学习解决方案。使用低代码 API 或无代码工作室来自定义、评估、原型设计和部署。<br>
            支持多种跟踪：人脸检测、手势跟踪、全身识别、头发识别、物体检测.
        - **地址**：[MediaPipe](https://developers.google.com/mediapipe)
    - kalidokit
        - **特点**：适用于 Mediapipe/Tensorflow.js 面部、眼睛、姿势和手指跟踪模型的 Blendshape 和运动学计算器。
        - **地址**：[kalidokit](https://github.com/yeemachine/kalidokit)

## 4. 开源例子

上面讲单项技术而将各项技术组织成真正产品形态需要一定的技术熟悉度。这里给几个开源项目例子，可以作为熟悉技术的参考。

### SysMocap
- **特点**：动作捕捉与虚拟形象同步
- **技术**：使用由Mediapipe进行面部、身体姿势的动作捕捉，使用Kalidokit进行Mediapipe的操作。
- **地址**：[SysMocap](https://github.com/xianfei/SysMocap)

### Linly-Talker
- **特点**：在可以支持情感迁移的视频文件生成类的AI
- **技术**：Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。使用Gradio实现AI算法可视化部署与交互。
- **地址**：[Linly-Talker](https://github.com/Kedreamix/Linly-Talker)

### metahuman-stream
- **特点**：以视频流的方式与前端进行实时交互。
- **技术**：使用ER-NeRF的图片渲染方式进行视频帧的渲染与流式视频生成。
- **地址**：[metahuman-stream](https://github.com/lipku/metahuman-stream.git)

### talking_avatar
- **特点**：使用前端3D模型展示方式进行虚拟形象展示，并进行动画操控方式生成虚拟形象唇形与动作。不需要后端进行视频流的生成，减少后端的运算压力。
- **技术**：使用GLTF方式进行模型的加载与渲染，Animation使用Blendshapes方式进行模型动作的驱动。使用微软云的Viseme服务来控制唇形与模型的同步。
- **地址**：[talking_avatar前端](https://github.com/pavlo91/talking-avatar.git)、[talking_avatar后端](https://github.com/bornfree/talking_avatar_backend.git)

## 5. 总结

本文从业务需求到具体实现技术、再到开源项目实例，全方面的给了一套虚拟形象的解决方案。可以用这套方案去进行虚拟形象的采购评估，也可以从技术角度去进行虚拟形象的技术实现。其实一篇文章是不可能说明虚拟形象所有的问题的，这里只是抛砖引玉，希望对大家有所帮助。

### 人工智能开源社区
在本文中隐约会发现在人工智能方向有几个非常常用的网站： [魔塔社区](https://modelscope.cn) ， [Hugging Face](https://huggingface.co/) 。这两个网站可以作为人工智能技术的的开源基地对人工智能技术的学习、应用、分享提供了基础设施。而且还提供各种类型的预训练模型，训练模型所需的数据集。这其实解决了很多在学习、训练过程中收集数据、数据打标签的麻烦。

### 恐怖谷问题
虚拟形象有一个很现实的问题：“恐怖谷效应”。其实就是虚拟形象与人之间的相似度在很像、又一眼能看出来不是人的阶段看起来就很恐怖。这时候最好不要停留在这个相似度上，最好调节动作要协调、表情自然度、声音与唇形的同步已达到更好的相似度。

### 云基础服务
从本文中可以看到虚拟形象其实设计到很多中类型的人工智能。而每种人工智能都需要大量的GPU作为运算能力，现阶段GPU设施的费用对于CPU设施来说还是很贵的。可以使用云服务商的人工智能运行环境来训练与运行自己的人工智能模型，也可以直接使用云服务商虚拟形象服务来完成虚拟形象的搭建。在国内的云服务商中对于开源支持比较好的是： [百度飞桨](https://www.paddlepaddle.org.cn/) 。项目开源，云支持。可以轻松的进行上下云的操作，为人工智能的搭建提供了更好的自由度。

### 适合最重要
从功能、训练时间、正确率、计算能力需求、存储几个角度对**大模型和小模型进行对比**。大模型的功能多、参数多意味着复杂度大，复杂度高意味着计算量大、存储量大。这样就意味着要用大模型去完整一些工作的费用就远大于小模型。在基于边际效应递减原则，费用的提升对正确率的提升其实达不到它能带来的收益提升。所以，选择模型的过程中要：适合的才是最好。

## 6. 参考
[动手学深度学习](https://zh.d2l.ai/index.html) <br>
[《PyTorch深度学习实践》完结合集](https://open.163.com/newview/movie/free?pid=DHSMAJQTM&mid=RHSMBLAED) <br>
[PaddleAvatar](https://zhuanlan.zhihu.com/p/628933165) <br>
[什么是视素(Viseme)？该如何表示？](https://www.zhihu.com/question/630593484/answer/3292854657) <br>
[前瞻交互：从语音、手势设计到多模融合](https://book.douban.com/subject/35721566/) <br>